# 逻辑回归

逻辑回归是一个分类算法，听着可能有点矛盾

逻辑回归就是在解释为什么逻辑“回归”是一个分类算法

但是线性回归很容易受到离群点的影响

## Sigmoid 函数

把函数“掰弯”成符合我们要求的函数
$$
S(h_\theta(x))=\frac{1}{1+e^{-h_\theta(x)}}=\frac{1}{1+e^{-\theta^T(x)}}\\
\lim_{\rightarrow\infin}\frac{1}{1+e^{-h_\theta(x)}}=1,\lim_{\rightarrow-\infin}\frac{1}{1+e^{-h_\theta(x)}}=0
$$
sigmoid 函数在整个机器学习和深度学习当中都有比较重要的地位

逻辑回归的损失函数
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}y(logS_{\theta}(x^{(i)})+(1-y^{(i)})log(1-S_{\theta}(x^{(i)}))
$$
更新参数$\theta$:
$$
\theta_{i+1}=\theta_{i}-\alpha\frac{\partial}{\partial\theta}J(\theta_i)\\
until:J(\theta_{i+1})-J(\theta_i)<th
$$
求导得：
$$
\theta_{i+1}=\theta_{i}-\alpha\sum_{i=1}^m(S_{\theta}(x_i)-y_i)x_i\\
until:J(\theta_{i+1})-J(\theta_i)<th
$$

## 逻辑回归的优缺点

优点：

- 输出结果是一个概率

- 不易被少量离群点干扰

缺点：

- 计算量较大
- 处理不好多特征变量
- 容易欠拟合
- 不支持多分类

重要性：

虽然复杂和高精度的项目中很少使用逻辑回归，但逻辑回归的思想是整个机器学习/深度学习中最重要的概念之一

## 多分类

某一件事发生的概率输出的是一个值（2分类）

但很多现实情况下，都是要去判断过得事件各自发生的概率，这时输出的是多个值（多分类）

通常采用 **One-vs-All**，亦称 **One-vs-the Rest** 方法来实现多分类

还有 one-versus-one (OvO)

OVR 和 OVO 都是用逻辑回归构造的多个二分类器

SOFTMAX 是逻辑回归二分类的拓展

