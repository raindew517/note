# 支持向量机

It is not the mountain we conquer but ourselves. -- Edmund Hillary

支持向量机（Support Vector Machine）是 Cortes 和 Vapnik 于 1995 年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势

它有很准确的特点，但是理解起来很费劲，它有很多公式

## 分类间隔

分类边界的构造其实只与少数样本有关

间隔越大，说明分类置信度越高

几何间隔：
$$
\gamma^{(i)}=\frac{w^Tx^{(i)}+b}{||w||},i=1,2...n
$$
函数间隔：
$$
\hat{r}^{(i)}=y^{(i)}(w^Tx^{(i)}+b),i=1,2...n
$$
当 $||w||=1$ 时，函数间隔和几何间隔相同

最优的分类超平面：
$$
max_{w,b}\frac{2}{||w||}\\
min_{w,b}\frac{||w||^2}{2}
$$
同时还要保证分类正确：
$$
y^{(i)}(w^Tx^{(i)}+b)≥1, i=1,2...n
$$

## 对偶问题与KKT条件

带有约束的优化问题就要用到拉格朗日

**拉格朗日乘子法**是一种经典的求解条件极值的解析方法，可将所有约束的优化模型问题转化为无约束极值问题的求解
$$
min_{w,b}\frac{||w||^2}{2}\\
s.t.-y^{(i)}(w^Tx^{(i)}+b)-1\le0,\ i=1,2...n
$$
定义拉格朗日系数 $a_i(i=1,2,3...n)$

乘以约束函数并与目标函数相加得到如下的拉格朗日函数
$$
L(w,b,a)=\frac{||w||^2}{2}-\sum_{i=1}^na_i(y^{(i)}(w^Tx^{(i)}+b)-1)
$$
其中 $a_i\ge0,i=1,2,3...n$

因为满足约束条件的 w，b 会使得 $y^{(i)}(w^Tx^{(i)}+b)-1$ 为零

不满足约束条件时，$-\sum_{i=1}^na_i(y^{(i)}(w^Tx^{(i)}+b)-1)\ge0$

因此 $L(w,b,a)$ 可取正无穷（让 a 为正无穷即可），最小化 $L(w,b,a)$ 就成了一个无解的问题

现在问题变成了这样
$$
\frac{||w||^2}{2}=\max\limits_{a\ge0}L(w,b,a)\ge L(w,b,a)
$$
必须满足约束条件的情况下，将目标转化为一个无约束问题（融合约束条件）
$$
\min\limits_{w,b}\frac{||w||^2}{2}=\min\limits_{w,b}\max\limits_{a\ge0}L(w,b,a)
$$
这个仍然不好求解，于是转化为它的对偶问题

- 对偶问题的对偶是原问题
- 无论原始问题是否是凸的，对偶问题都是凸优化问题
- 对偶问题可以给出原始问题一个下届
- 当满足一定条件时，原始问题与对偶问题的解释完全等价的

$$
p^*=\min\limits_{w,b}\max\limits_{a\ge0}L(w,b,a)\ 原问题\\
d^*=\max\limits_{a\ge0}\min\limits_{w,b}L(w,b,a)
$$

弱对偶性：$d^*\le p^*$ 对于任何优化问题成立

强对偶性：$d^*=p^*$ 满足KKT条件时成立

KKT条件：
$$
L(w,b,\alpha)对w和b偏导为0\\
\alpha\ge 0\\
y^{(i)}(w^Tx^{(i)}+b)-1\ge0\\
\alpha(y^{(i)}(w^Tx^{(i)}+b)-1)=0
$$

满足上面的条件，对偶问题就等于原问题了

对函数求偏导（分别对 w 与 b），并令其为0
$$
\frac{\partial L(w,b,\alpha)}{\partial w}=w-\sum_{i=1}^na_iy^{(i)}x^{(i)}=0\\\Rightarrow w=\sum_{i=1}^na_iy^{(i)}x^{(i)}\\
\frac{\partial L(w,b,\alpha)}{\partial b}=\sum_{i=1}^na_iy^{(i)}=0
$$
其实就是为了用 $\alpha$ 来代替 w 和 b
$$
L(w,b,a)=\frac{||w||^2}{2}-\sum_{i=1}^na_i(y^{(i)}(w^Tx^{(i)}+b)-1)\\其中\alpha_i\ge0,i=1,2,3...n
$$
就变成了
$$
L(w,b,a)=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i,j=1}^n(a_ia_jy^{(i)}y^{(j)}(x^{(i)})^Tx^{(j)})\\其中i,j=1,2,3...n\\
s.t\sum_{i=1}^na_iy^{(i)}=0,\alpha_i\ge0
$$

## 核函数

通过核函数的让线性分类器解决非线性问题

通过核方法，也许可以将数据本身映射到更高维度的空间中去

在高纬度的空间中，也许通过一个线性的方法就可以解决我们的问题

定义核函数
$$
K(x_{train},x_{test})=\Phi(x_{train})^T\Phi(x_{test})=<\Phi x_{train},\Phi x_{test}>
$$
关键问题： 求 $K(x_{train},x_{test})$ 也就是求训练集点与测试点在映射之后高维度上的内积
$$
\Phi(x_{train},x_{test})=\sum_{i,j=1}^n(x_{train_i}x_{test_j})(x_{train_i}x_{test_j})\\
K(x_{train},x_{test})=((x_{train})^Tx_{test})^2\\
m是数据的维度（原始特征数量）
$$
$\Phi$ 是非核方法，K 是核方法

核函数蕴含了从低维到高维的映射思想，从而避免直接计算高维的内积

非核方法是先将原始数据投影到更高的维度上才去计算他们的内积

核方法是先计算他们的内积在投影到更高的维度上去，不需要每次具体计算出原始样本点映射的新的无穷维度的样本点，直接使用映射后的新的样本点的点成计算公式即可

- 减少计算量
- 减少存储空间

高斯核（可映射到任意维）：
$$
K(x_{train},x_{test})=exp(-\frac{||x_{train}-x_{test}||^2}{2\sigma^2})
$$
多项式核（可映射到n维）：
$$
K(x_{train},x_{test})=(\gamma<x_{train},x_{test}>+c)^n
$$
线性核（其实就是线性可分svm）：
$$
K(x_{train},x_{test})=<x_{train},x_{test}>
$$
原来的分类函数就变成了下面的样子
$$
\max\limits_{\alpha}(w,b,\alpha)=\max\limits_{\alpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^n(\alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)}))\\
s.t\sum_{i=1}^n\alpha_iy^{(i)}=0,\alpha_i\ge0\\
wx+b=(\sum_{i=1}^n\alpha_iy^{(i)}x^{(i)})^T+b=\sum_{i=1}^n\alpha_iy^{(i)}K(x^{(i)},x)+b
$$

## 软间隔和正则化

当样本点中有一些离群点的时候，我们只能近似线性可分，要有意地去忽略那些离群点才可以进行分类

当考虑所有的点的时候构造**硬间隔**可能是无解的

如果能忽略掉噪声点（离群点）
$$
y^{(i)}(w^Tx^{(i)}+b)\ge1-\xi\\
\xi 是松弛变量，\xi\ge0
$$
这样的间隔我们称之为**软间隔**，因为它忽略了一些点

对于点 (i=1...n)，不要求所有的点都能满足 $y^{(i)}(w^Tx^{(i)}+b)\ge1$，而是对于少数的点，可以放宽条件构造**软间隔**
$$
\min\limits_{w,b}\frac{1}{2}||w||^2\\
s.t.y^{(i)}(w^Tx^{(i)}+b)\ge1-\xi\ ,\ i=1,2...n
$$
**好处**：避免了分类面向个别点移动，使得分类面间隔更大，或者从无解到有解

**坏处**：丢弃了对某些点的精确分类，使得分类起整体受到了损失

所以需要去平衡好处和坏处
$$
\sum_{i=1}^n\xi_i\  \sum_{i=1}^n\xi_i^2\\
\min\limits_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_{i=1}^n\xi_i^2\\
s.t.y^{(i)}(w^Tx^{(i)}+b)\ge1-\xi\ ,\ i=1,2...n
$$
惩罚系数C是一个超参数，不是求解的目标

惩罚系数C越大，说明我们越不愿意放弃这些离群点，惩罚系数越小，说明我们对于这些点的死活越不在乎

将其代入拉格朗日乘法中
$$
L(w,b,\xi,\alpha,\beta)=\frac{||w||^2}{2}+C\sum_{i=1}^n\xi_i-\sum_{i=1}^n\alpha_i(y^{(i)}(w^Tx^{(i)}+b)-1+\xi)-\sum_{i=1}^n\beta_i\xi_i\\
其中\alpha_i\beta_i\ge0,i=1,2,3...n\\
\min\limits_{w,b,\xi}\frac{1}{2} 
$$

$$
\max\limits_{\alpha}L(w,b,\xi,\alpha,\beta)=\max\limits_{\alpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^n(\alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)}))\\
s.t\sum_{i=1}^n\alpha_iy^{(i)}=0,C\ge\alpha_i\ge0\\
$$

## SMO 算法

将软间隔和核函数结合，就得到了 SVM 的完全体
$$
\max\limits_{\alpha}L(w,b,\xi,\alpha,\beta)=\max\limits_{\alpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^n(\alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)}))\\
s.t\sum_{i=1}^n\alpha_iy^{(i)}=0,C\ge\alpha_i\ge0\\
$$
但是现在 $\alpha$ 直接求似乎很难求，所以要用到 smo 算法

SMO优化算法（Sequential minimal optimization）

求解 $\alpha$ 的过程是一个二次规划+启发式算法的过程
$$
\alpha=[\alpha_1,\alpha_2,\alpha_3,...\alpha_i]\\
\alpha=[\alpha_k,\alpha_m,constant(other\ \alpha_i)]
$$
每回就去求一对 $\alpha$ ，每一轮 $\alpha$ 都要满足 KKT 条件，每轮优化一对 $\alpha$ 直到结束

所以现在的问题就变成了如何选择该轮优化的 $\alpha$，以及如何优化该轮的 $\alpha$
$$
\max\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^n(\alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)}))\\
s.t\sum_{i=1}^n\alpha_iy^{(i)}=0,C\ge\alpha_i\ge0
$$
变成下面
$$
\min\frac{1}{2}\sum_{i,j=1}^n(\alpha_i\alpha_jy^{(i)}y^{(j)}K_{ij})-\sum_{i=1}^n\alpha_i\\
s.t\sum_{i=1}^n\alpha_iy^{(i)}=0,C\ge\alpha_i\ge0
$$
假设这一对是第一个和第二个 $\alpha$，对目标函数展开
$$
\min\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+\alpha_1\alpha_2y_1y_2K_{12}-(\alpha_1+\alpha_2)\\
+\alpha_1y_1\sum_{i=3}^N\alpha_iy_iK_{1i}+\alpha_2y_2\sum_{j=3}^N\alpha_jy_jK_{2j}+c\\
上边是第一个和第二个\ \alpha，下面的是剩余的\ \alpha\\
其他的 \alpha 就全部变成了常数 c
$$
约束条件就从
$$
s.t\sum_{i=1}^n\alpha_iy^{(i)}=0,C\ge\alpha_i\ge0
$$
变成了
$$
\alpha_1y_1+\alpha_2y_2+\sum_{i=3}^n\alpha_iy_i=0,C\ge\alpha_i\ge0\\
\alpha_1y_1+\alpha_2y_2=-c,C\ge\alpha_i\ge0\\
$$
即使是这样，求两个 $\alpha$ 仍然是很麻烦的求解，所以在这对 $\alpha$ 中，还要想办法让其中一个表示另一个，根据 $\alpha_1y_1+\alpha_2y_2=c$ （c一下是正的，一下是负的就很迷）就可以知道了

但这里的 $y=\pm1$ 没看太明白





## 相关 API 与超参数

c：惩罚系数，越大表示也不愿意放弃离群点，实际过程中就是太大了也不会报错，它会把所有的样本都归为一类

kernel：支持向量机核函数的种类，常用的高斯核、多项式核和线性核，最常用的就是高斯核，基本上是默认的选择，使用多项式核的时候需要指定多项式的系数，使用线性核默认为线性SVM

degree：使用多项式核的时候指定的多项式系数，越大则有可能有越好的分类边界，但也可能陷入过拟合，以及实际使用中计算的异常缓慢

probability：返回值是概率还是类别，true 是概率

tol：求解时的容忍度，也是精度，值得是求解当前那一对 $\alpha$ 时的精度，越大则潜在的迭代次数越少，解有可能不精确，太小的话迭代次数可能很多，计算量可能很大

cache_size：计算时的内存分配量，单位是 mb

class_weight：类别权重，用来解决样本不平衡问题

decision_function_shape：多分类策略，ovo 还是 ovr，建议 ovr，SVM 本身就很慢了，用 ovo 会更慢，sklearn 前面很多的算法默认是 ovo，但在 SVM 中就是默认 ovr

## 碎碎念

优点：

- 少量“支持向量”提供分类依据，避免了冗余信息
- 很好的非线性效果
- 鲁棒性较强
- 避开了统计学方法，意味着少量样本也可以精确计算
- svm 是一种非常美妙，甚至数学上“完美”的方法

缺点：

- 慢，非常慢，意味着不支持大规模训练
- 难，非常难，意味着学习和掌握成本太高

svm 方法其实也不能是很严格的机器学习方法，因为它产生的解都是解析解，现在 svm 已经算是被淘汰的算法了，但是却是很适合挑战自己的算法，可以用了挑战一下自己

| SVM 的间隔     | 目标函数            | 拉格朗日乘子求解 | 线性 SVM 的最终形式 |
| -------------- | ------------------- | ---------------- | ------------------- |
| 核函数与映射   | 带核函数的 SVM 形式 | 软间隔，惩罚系数 | SVM 的最终优化形式  |
| 如何求解一对 α | 如何选择一对 α      | SVM 的优化流程   | 超参数与 API        |

