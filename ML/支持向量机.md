# 支持向量机

It is not the mountain we conquer but ourselves. -- Edmund Hillary

支持向量机（Support Vector Machine）是 Cortes 和 Vapnik 于 1995 年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势

它有很准确的特点，但是理解起来很费劲，它有很多公式

## 分类间隔

分类边界的构造其实只与少数样本有关

间隔越大，说明分类置信度越高

几何间隔：
$$
\gamma^{(i)}=\frac{w^Tx^{(i)}+b}{||w||},i=1,2...n
$$
函数间隔：
$$
\hat{r}^{(i)}=y^{(i)}(w^Tx^{(i)}+b),i=1,2...n
$$
当 $||w||=1$ 时，函数间隔和几何间隔相同

最优的分类超平面：
$$
max_{w,b}\frac{2}{||w||}\\
min_{w,b}\frac{||w||^2}{2}
$$
同时还要保证分类正确：
$$
y^{(i)}(w^Tx^{(i)}+b)≥1, i=1,2...n
$$

## 对偶问题与KKT条件

带有约束的优化问题就要用到拉格朗日

**拉格朗日乘子法**是一种经典的求解条件极值的解析方法，可将所有约束的优化模型问题转化为无约束极值问题的求解
$$
min_{w,b}\frac{||w||^2}{2}\\
s.t.-y^{(i)}(w^Tx^{(i)}+b)-1\le0,\ i=1,2...n
$$
定义拉格朗日系数 $a_i(i=1,2,3...n)$

乘以约束函数并与目标函数相加得到如下的拉格朗日函数
$$
L(w,b,a)=\frac{||w||^2}{2}-\sum_{i=1}^na_i(y^{(i)}(w^Tx^{(i)}+b)-1)
$$
其中 $a_i\ge0,i=1,2,3...n$

因为满足约束条件的 w，b 会使得 $y^{(i)}(w^Tx^{(i)}+b)-1$ 为零

不满足约束条件时，$-\sum_{i=1}^na_i(y^{(i)}(w^Tx^{(i)}+b)-1)\ge0$

因此 $L(w,b,a)$ 可取正无穷（让 a 为正无穷即可），最小化 $L(w,b,a)$ 就成了一个无解的问题

现在问题变成了这样
$$
\frac{||w||^2}{2}=\max\limits_{a\ge0}L(w,b,a)\ge L(w,b,a)
$$
必须满足约束条件的情况下，将目标转化为一个无约束问题（融合约束条件）
$$
\min\limits_{w,b}\frac{||w||^2}{2}=\min\limits_{w,b}\max\limits_{a\ge0}L(w,b,a)
$$
这个仍然不好求解，于是转化为它的对偶问题

- 对偶问题的对偶是原问题
- 无论原始问题是否是凸的，对偶问题都是凸优化问题
- 对偶问题可以给出原始问题一个下届
- 当满足一定条件时，原始问题与对偶问题的解释完全等价的

