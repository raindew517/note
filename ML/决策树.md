# 决策树

life is full of possibilities, each time you make a decision, it settles a bit. -- Nemo

## 信息增益

熵是热力学中表征物质状态的参量之一，其物理意义是体系混乱程度的度量

信息熵，信息学中表示一个体系内信息的混乱度（不确定性程度），信息熵越大，说明信息的不确定性越大，信息熵越小，说明体系的信息越明确，信息熵为 0 时，体系为确定事件

在数学中可以这样表示：
$$
Info=-p\log_2p\\
P-{p_1,p_2,p_3...p_n}\\
Info(D)=-\sum_{i=1}^mp_i\log_2(p_i)
$$
信息增益 `Information Gain = entropy(before) - entropy(after)`

$$
g(D,B)=H(D)-H(D|B)\\
信息增益=原始信息熵-条件熵
$$

## ID3 算法

ID3 (Iterative Dichotomiser 3) 算法诞生于 1975 年，算法进行树构建的**核心**依据：**信息增益** information gain（由**信息熵**计算而来）**分裂节点时，优先选择增益搭的特征**，它有点像贪心算法，每次只会选择当下最优解，而不是根据整体去选择最优解，所以它得到的很可能是局部最优解
$$
Info(D)=-\sum_{k=1}^m\frac{|C_k|}{|D|}\log_2(\frac{|C_k|}{|D|})
$$
ID3 算法只能计算离散型的条件，而且在条件都只有一个样本点的时候是没有意义的，它叫迭代二叉树，说明也就只能处理离散型变量

## C4.5 算法

用信息增益比来替代信息增益，可以构建多叉树，**将连续性变量转化成离散型变量去处理**

因为在用信息增益来求解得时候，我们忽略了计算条件熵中条件的内部信息混乱程度（熵），实际上衡量总体信息得失的量化过程中，应该包含**条件本身的混乱程度**（熵），但在信息增益的定义当中却没有进行计算

所以在 C4.5 算法中使用了信息增益比（information Gain Ratio）

 **信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。**
$$
IGR(D,A)=\frac{Ent(D)-Ent(D|A)}{Int(A)}\ where\ Int(A)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2(\frac{|D_i|}{|D|})\\
分母的 Int(A) 就是内部混乱度
$$
有时候也有这样的形式：
$$
IG(D,A)=C*(Ent(D)-Ent(D|A))\\
C 是惩罚系数，来代替内部混乱程度
$$
如果有这样的数据：

|      | coding | speaking | Late_work | drink |
| ---- | ------ | -------- | --------- | ----- |
| day1 | 180    | 15       | 0         | 1     |
| day2 | 177    | 42       | 0         | 0     |
| day3 | 136    | 35       | 1         | 0     |
| day4 | 174    | 65       | 0         | 1     |
| day5 | 141    | 28       | 1         | 0     |

将连续性变量**离散化**，将 $(x_1^{feature},......,x_Z^{feature})$ 排序后划分出 Z-1 个节点，每个节点的数值为
$$
c_i=\frac{x_i^{feature}+x_i^{feature}}{2}
$$
计算每个特征的最大划分，再从中选择全局最大作为该节点的划分

依据本节点(大于或者小于本节点)，将数据集划分为两段(离散化)，然后根据当作离散特征计算信息增益比 $(x_1^{feature},......,x_i^{feature})$ $(x_{i+1}^{feature},......,x_Z^{feature})$ 计算该特征的 Z-1 个信息增益比，选择最大的该特征划分

分割点会将数据分成两段，这就变成了只有两个值得离散情况，但是有很多个分割点，也就是说会有很多种情况，就是要计算哪种情况的信息增益比最大，然后取那种情况的分割点

前面说到 C4.5 可以构建多叉树，但是也只有在数据已经是离散型且多于两个值的情况下，如果是连续型的变量是不构造成多叉树的

终结一下 C4.5 算法的流程就是：

1. 计算当前节点的类别熵 Info(D)
2. 计算当前节点的属性熵 Info(Ai)（按照属性取值下的类别取值计算）
3. 计算各个属性的信息增益 Gain(D) = Info(D) - Info(Ai)
4. 计算各个属性的分类信息度量 H(Ai)
5. 计算各个属性的信息增益率

## 基尼系数与 CART 算法

基尼系数是用来描述收入分配平均的数
$$
Gini(p)=\sum_{i=1}^np_i(1-p_i)=1-\sum_{i=1}^np_i^2
$$
与信息熵、信息增益不同的是，基尼系数越小，则特征越好

### CART 算法

Classification And Regression Tree 分类回归树算法

用**基尼系数**替代**信息增益、信息增益比**，某一个特征如果比较重要的话，可以重复利用多次，构建的是**二叉树**

可以处理**回归问题**

在处理回归问题的时候，它用的策略和 C4.5 是一样的，都是连续变量离散化

与 ID3 和 C4.5 不同的是，在计算时不适用 GINI/IG/IGR（标签是连续性，无法统计概率），而使用 MSE（均方差），使用均方差最小的作为分裂依据
$$
MSE=\sum_{i=1}^m(f(x^{(i)})-c^{(i)})^2
$$
其中 c 为该区域内标签值得平均值

## 决策树的剪枝

机器学习中的决策树过度生长会导致**过拟合**

极端情况下，每个叶节点强行记忆数据集中的每个样本，彻底失去泛化能力

有两种策略：预剪枝和后剪枝

**预剪枝**是形成决策树之前预先规定数的生长方式

**后剪枝**是形成决策树之后减除多余节点

### 预剪枝

限制树的深度

限制叶的数量

限制分裂特征的阈值，如果信息重要程度连阈值都达不到，就不作为特征参考

叶中样本最小数量，如果叶中的样本太少了，可以考虑和别的叶子合并

其实每个数据集都可能是很独特的，所以我们一般也不太容易做到预剪枝

### 后剪枝

形成决策树之后减除多余节点

**核心思想：剪前、剪后评估指标**

就是如果剪枝后的效果更好就剪枝，否则就不剪，这个衡量的标准和其他机器学习的标准一样，都是损失函数
$$
C_\alpha(T)=C(T)+\alpha|T|\\
C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\\
|T|\ 叶节点个数\\
N\ 对应叶节点的样例数\\
H\ 对应叶节点经验熵表示叶节点经验熵
$$

## 特性汇总

缺失值处理
$$
I(D|A)=\frac{A_{notnull}}{A_{all}}*(I(D)-I(D|A_{nt}))
$$

|      | model      | tree | 特征                                         |
| ---- | ---------- | ---- | -------------------------------------------- |
| ID3  | 分类       | 二叉 | 不支持连续特征<br />不支持缺失<br />不能剪枝 |
| C4.5 | 分类       | 多叉 | 支持连续特征<br />支持缺失<br />支持剪枝     |
| CART | 分类与回归 | 二叉 | 支持连续特征<br />支持缺失<br />支持剪枝     |

优点：

- 可解释性强
- 非线性
- 没有太复杂的数学过程（相对）
- 结构化数据处理优秀
- 对缺失值不敏感
- 不需要标准化
- 使用时计算量小
- 可以处理混合数据

缺点：

- 容易过拟合（可解决）
- 实际使用中需要大量数据
- 需要较大数据/类别比
- 容易被不平衡样本影响（可解决）

## 相关的 API 与超参数

在 sklearn 中其实都是默认 cart 算法，因为 ID3 和 C4.5 都已经湮灭在历史的长河中了，但是也可以通过修改一些参数来实现他们

**criterion** : gini(基尼系数) or entropy(信息熵)

下面的参数大部分都是预剪枝的作用

**max_depth**: 决策树最大深度（层数），太少会欠拟合，太多会过拟合，默认不限制

**min_samples_split**: 节点样本最小值，小于这个值就不会继续划分了。如果样本数量很大，请酌情增大这个参数。

**min_samples_leaf**: 叶子的样本最小值，如果小于这个值就会被剪枝，样本量很大的话可以酌情增大。

**max_features**: 默认使用多少个特征来进行分类，按照criterion分裂，如果已经使用了这个数量的特征，剩下的特征就不会再分裂

**max_leaf_nodes**: 最大叶节点，防止过拟合，特征多请考虑限制叶的数量

**min_impurity_split**: 分裂时需要的不纯度（信息增益/比/基尼系数），过小会导致过拟合，过大会欠拟合或者无法分类

**class_weight**: 决策树容易被不平衡样本影响，可以考虑增大对应类别的权重，回归中就没有这个参数

## 总结

- 树的结构
- 构建树的方法-分裂
- 信息熵与信息增益
- ID3算法如何分裂特征构建决策树
- 信息增益比
- 特征连续变量离散化
- C4.5算法如何分裂特征构建决策树
- 基尼系数
- CART算法如何构建分类树
- CART算法如何构建回归树
- 预剪枝与后剪枝
- 缺失值处理
- 决策树优点与缺点
- 几种决策树的异同
- 超参数与API

