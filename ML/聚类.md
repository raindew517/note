# 聚类

You cannot eat a cluster of grapes at once, but it is very easy if you eat them one by one.

-- Jacques Roumain

虽然现在大多是监督学习，但是其实现实生活中大多是没有 label 的数据

没有标注的原因：

1. 没有足够的资源标注
2. 没有足够的先验知识

**距离度量**

欧氏距离(L2)，就日常的两点之间的距离
$$
\sqrt{\sum_{i=1}^k(A_i-B_i)^2}
$$
曼哈顿距离(L1)，沿着空间坐标轴行径的长度
$$
\sum_{i=1}^k|A_i-B_i|
$$
明氏距离(Lq)
$$
({\sum_{i=1}^k(A_i-B_i)^q})^{1/q}
$$
明氏距离在q=1时就是曼哈顿距离

明氏距离在q=2时就是欧氏距离

一般欧氏距离就够用了

在算的时候可能不同维度之间的差距很大，比如特征一都是几百上千，特征二可能就是零到一，这个时候我们就可以在训练钱就对数据进行标准化，不至于到时候算曼哈顿距离某些特征被放大，某些特征被忽视，因为特征之间都在相同的数量级才能更好的算嘛

L1 和 L2 距离的区别：

- 绝大多数时候我们使用 L2
- 绝大多数时候 L2 和 L1 结果很相似
- L1 在高维度可能比 L2 更加优秀
- L1 可能更加不容易受离群点影响

还有很多其他距离

- Chebyshev Distance
- Mahalanobis Distance
- Cosine Distance
- Hamming Distance
- Jaccard Distance
- Correlation 相关性

**损失函数（性能评估）**
$$
J=\frac{1}{m}\sum_{i=1}^{N}||x^{i}_z-c^{i}_z||^2
$$
即本聚类中心到本类中所有样本距离的平均值

显然这个数值越小，聚类的选取就越正确

还有一些其他的评估：

- Davies-Bouldin Index
- Dunn Index
- Calinski-Harabaz
- Silouette coefficient
- FMI
- Ajusted Mutual Information

## K-Means 聚类（Centroid）

最出名的聚类方法，没有之一

1. 初始化 K 个聚类中心
2. 计算样本到聚类中心的距离并且更新样本所属簇
3. 更新聚类中心
4. 不断重复 2，3
5. 停止条件：聚类中心不再变更或者打到指定迭代次数

选取不同的初始点会影响到最后的结果，所以可以用损失函数来评估最后的分类结果

在商业的情况下，大部分都是根据商业目标去选择 K 的数值

K=1 和 K=样本数都是没有什么意义的

只有在不知道 K 选取多少的时候才会去用手肘法则

## 基于层次的聚类（Hierarchical）

1. 计算样本点的两两距离（计算量有点大）
2. 选取最短距离，合并簇
3. 形成了一个簇之后，再计算 Linkage，然后距离小的再合并

Linkage computation:

Single Linkage：
$$
L(1,2)=min(d(x_{1i},x_{2j}))
$$
Average Linkage

Complete Linkage:
$$
L(1,2)=max(d(x_{1i},x_{2j}))
$$

## 基于密度的聚类（Density）

以 DBSCAN 从的一些概念为例：

eps - 中心点的扫描半径

th(min_points) - 落在 eps 中点数量的最小阈值

如果中心店 eps 内点的数量大于等于 th：Core（核心点）

当前中心点 eps 内点的数量小于 th，但在某个 Core 的领域内：Border（边界点）

既不满足 Core 也不满足 Border 的中心点：Noise（噪点）

还有更多的概念：

**Reachability & Connectivity**

如果两个都是 Core 那两个点可以相互转化

如果只有一个是 Core，另一个不是 Core 而是它的边界点，那 Core 可以变成这个边界点，但是这个边界点不能变成这个 core

这说明了 Reachability 不是一个一定可逆的属性，但是它可以说明传播性，比如 c1，c2，c3都是核心点，相互之间可以到达，c4 不是核心点，但是其他点只要有一个点和 c4 是领域就可以到达 c4 

Connectivity：如果两个点没什么关系，但能同时由一个点到达的话，就说明这两个点相连

随机选择某个初始点，不选噪点

将所有可到达（reachable）和相连（connected）的点标记成一簇即可，然后提出这些点，再从剩下的子集中继续选点

## 总结

Centroid：需要先明确中心然后根据各个点慢慢去收敛中心

Hierarchical：从局部到中心的方法

Density：根据 eps 内的 reachability 和 Connectivity 合并

实际使用中还是第一种方法 Centroid 用的最多

### 特点

这里直说了每种方法中的一个，其实它们都有其它改进的方法

Centroid：

- 最常用的聚类方法
- 很好的适应性
- 只有一个重要超参数需要选择（k）
- 比较容易理解
- 初始点悬着和目标函数需要关注
- 对样本在样本空间的分布有要求
- 噪声和利群点会有较大影响

Hierarchical：

- 最自然的聚类方法
- 可以发现数据的内在层次关系
- 计算复杂度较高
- 一定程度上抗噪
- 样本的分布形状接受度较高

Density：

- 可以不选择聚类别数
- 很好的抗噪能力
- 初始点不需要关注
- 不好定义参数 eps 和 th，尤其是在高维情况下
- 可以接受任何样本的分布形状

## 超参数

KMeans

- n_clusters = 3,    聚类类别数量

- init = 'k-means++' , 'random'， k-means++是sklearn的默认方法，会更聪明的加速收敛，其实就是一种改进的随机选点法

- n_init = 10 ,kmeans算法的重复计算次数

- max_iter = 300，kmeans算法对某个类的迭代次数，不要和n_init混淆了

- tol = 1e-4，收敛所需要的最小阈值

返回值

- cluster_centers_ 最重要的一个返回值，标识出每个聚类中心

- labels_ 返回每个样本点所属的类别（无监督，所以没有已知类别，以数字1234区分）

AgglomerativeClustering

- n_clusters = 3,    聚类类别数量

- linkage = 'ward' , 'complete','average'

返回值

- labels_ 返回每个样本点所属的类别（无监督，所以没有已知类别，以数字1234区分）

DBSCAN

- eps ,可选，如果不写，sklearn会根据样本点之间的间距选择eps值（通常不靠谱）

- min_samples  = 就是我们的th，可选，如果不屑，sklearn会根据样本的分布选择（通常不靠谱）

返回值

- labels_ 返回每个样本点所属的类别（无监督，所以没有已知类别，以数字1234区分）

- core_sample_indices_，返回所有的核心点索引

返回的这些个 label 不是监督学习里的那种 label，无监督学习是没有 label 的，这是后期学习后标注上的 label