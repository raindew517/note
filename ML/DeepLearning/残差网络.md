- Batch Normalization
- 深度的革命
- 残差网络ResNet
- 残差模块Residual block
- 1x1 filter的作用

## Batch Normalization

对于逻辑回归,数据标准化能使得训练时模型更快速的学习w, b,收敛得更快

思考:能不能对神经网络每层的z进行标准化,从而使得每层的w和b能更快速的学习,使得网络收敛得更快?答案是肯定的，但是是对z进行标准化还是对a进行标准化还有争议

一个batch的训练数据有m个训练样本
$$
\mu=\frac{1}{m}\sum_iz^{(i)}\\
\sigma^2=\frac{1}{m}\sum_i(z^{(i)}-\mu)^2\\
z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}} \ 这里\varepsilon防止分母为0\\
\tilde{z}_{norm}^{(i)}=\gamma z_{norm}^{(i)}+\beta \ 这里为了不使标准化降低网络的表达能力\\
$$
最后将结果输入到激活函数中去

把每层神经网络的神经元的输入值Z的分布强行拉回到均值为0,标准差为1的标准正态分布,然后网络通过学习y和,增加网络表达能力

BN要做的就是先将隐层的z强行拉回均值为0方差为1的正态分布(然后通过y和B增强网络表达),可以减轻这些隐层值分布不停变化的程度,减少前面层的参数对本层的影响,使得每层更加独立得学习,使得网络训练更加稳定和快速

## 深度的革命

一般我们说的“层”，是指带,有权重参数的层,包括卷积层和全连接层。不包含池化层

网络变得很深以后,会出现严重的梯度消失,深度网络就变得更加难以训练了!

Residual block
$$
a^{[l]}\\
z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}\\
a^{[l+1]}=g(z^{[l+1]})\\
z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
其他网络a^{[l+2]}=g(z^{[l+2]}) \ 残差网络就是在这里不同\\
a^{[l+2]}=g(z^{[l+2]}+a^{[l]}) \ 把a^{[l]}加上再输入到激活函数中去
$$
残差网络就是不停重复这样的结构
$$
a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]})当w、b接近于0的时候\\
=g(a^{[l]}) = a^{[l]}
$$
所以即使加了两层网络，它的效果也至少不会比原来差

更深的瓶颈结构 bottle neck

前面用到的都是3\*3的卷积核，在 bottle neck 中用的是1\*1的卷积核， 在它大概50,100层的时候会使用到瓶颈结构

## 1x1 filter的作用

1x1的卷积可以实现通道数量的升维和降维,并且是低成本的特征变换(计算量比3x3或5x5小很多) ,是一个性价比很高的聚合操作

## 强化学习

- 强化学习的学习系统没有被直接告知应该做出什么样的行为
- 必须允许尝试,与环境交互之后获得奖励,使奖励最大化
- 当前行为影响即时奖励,还影响下一步及后续所有奖励

内容

- 智能体(agent)
- 状态(state)
- 行为(action)
- 奖励(reward)

奖励(reward)指导动作(action),动作(action) 影响状态(state)

目标:选择一系列动作使得奖励最大化