## 全连接网络的局限性

1. 全连接网络忽略了数据特征之间局部结构关系 （ 时间关系、 空间关系 ） 
2. 全连接网络的待训练参数量非常大，随着输入数据增加以 及神经网络的复杂化，训练起来非常困难 

忽略了局部关系，参数过多

如果要输入大小为一张28\*28的单通道图片，只考虑一个隐藏层(16个神经元)的情况下，参数数量有多少?

每一个输入神经元会与相邻隐藏层中的所有神经元相连，所以会有28\*28\*16个连接，每个连接都会有一个权重参数w,且隐藏层中每个神经元会有一个偏置参数b

刚刚情景中，如果隐藏层神经元个数变为512个呢?28\*28\*512+512 =401920

如果再把输入图像换成1024*1024的图片呢?1024\*1024\*512+512 =536871424

于是我们需要卷积神经网络

## 卷积神经网络

生物学家通过对猫的视觉皮层细胞研究提出Receptive Field(感受野)，即每1个视觉神经元只会处理一小块区域的视
觉图像
像素点一>线条一>轮廓一>局部一>整体

所有的图像都可以被拆分成很多特征，任由图像中的主体旋转、跳跃、闭没闭眼，这些特征都不会改变

对于不同个体来讲，虽然个体之间有差异，但是他们可能拥有同样的特征。特征又由多个边缘组成，所以我们如果能够识别出很多种边缘，就能够将其组合成为各种不同的特征从而组合成图像

### 卷积

图像中不同位置的数据段与卷积核(一种滤波矩阵)做内积的操作，被称为卷积，在数字图像处理中也被称之为滤波(filter) ， 它的本质就是提取图像不同的特征。

### 卷积核

卷积核是带有一组权重的神经元，通常是二维的矩阵，相当于神经元的感受野。一个卷积核可以提取某种特定的特征，如物体轮廓、纹路等等。

### 卷积层

多个卷积核联合对同一张特征图进行特征提取，生成新的特征图，这些卷积核组成了卷积神经网络中的一个卷积层。

## 边缘检测

滤波器：prewiit，sobel，scharr

待卷积的图片叫特征图，滤波器就是卷积核，特征图和卷积核内积就会得到新的特征图

大小为n\*m的特征图，通过大小为f\*f的卷积核进行卷积之后，生成的特征图大小为多少呢?

(n-f+1)\*(m-f+1)

## Padding

按上面的方法看，卷积层数多起来后，得到的特征图就越来越小了

边缘数据利用率还低

于是我们引入了 padding 来解决上面两个问题

根据是否padding，卷积可以被分为两种形式Valid和Same
Valide ：即不进行padding (n,n) * (f,f) 得到 (n-f+1,n-f+1)
Same : (n+2p,n+2p)\*(f,f) 得到 (n+2p-f+ 1,n+2p-f+1)
要使 n+2p-f+1 = n 即 p=(f+1) /2

## Stride

之前每次步长都是1

对于(n,n) 的图像，使用(f, f)的卷积核，取padding为p，步长为s时，得到的输出为:
$$
(\lfloor\frac{n+2p-f}{s}+1\rfloor,\lfloor\frac{n+2p-f}{s}+1\rfloor)\\\lfloor\rfloor 是向下取整的意思，因为有的时候还剩一点像素，但是移动一步的步长后又不够
$$

## 多通道卷积

图片一般还是彩色的，所有要有多通道卷积
$$
input:(n_h,n_w,n_c^{[l-1]})\\
filter:(f,f,n_c^{[l-1]})\\
num\ of\ filter:n_c[l]\\
padding:p\\
stride:s\\
(\lfloor\frac{n+2p-f}{s}+1\rfloor,\lfloor\frac{n+2p-f}{s}+1\rfloor,n_c^{[l]})
$$

## 池化

池化就是为了数据降维

池化层:压缩特征图，一来可以使特征图变小，减少计算; 二来可以压缩特征，提取主要特征

### Max Pooling

和卷积核有点像，不过不是和卷积核内积，而是取区域内值最大的，然后输出成一个单位，接着按照步长继续前进

### Average Pooling

最大池化关注的最大值更适用于突出的轮廓，平均池化关注的是底层纹理

卷积神经网络操作流程就是在之前的全神经网络前面加上了卷积层和池化

即 特征图->卷积层->池化->...->全连接层->输出

# 主流神经网络模型

我们之前搭建卷积神经网络的时候,经常会有疑惑:

该用多大的卷积核?卷积核的数量?神经网络该设计多深?

通常来说,在某些经典计算机视觉问题上表现良好的模型,在其他问题也表现良好

因为我们都是从

边缘识别（水平线、垂直线、45°）到提取特征再提取特征，然后特征组合再到识别物体，猫狗，人脸，自动驾驶

所以能用经典模型来：

1. 借鉴
2. 快速实现业务
3. 迁移

经典网络模型：

- LeNet-5：1998 年，CNN的“老祖宗”
- AlexNet：2012 年 ILSVRC 冠军，Deep Learning 崛起的标志
- VGG：2013 年 ILSVRC 亚军
- ResNet：2015 年 ILSVRC 冠军，Deep Learning 里程碑

## LeNet-5

LeNet-5出自Yann LeCun教授1998年论文 Gradient-BasedLearning Applied to Document Recognition,是一种用于手写体字符识别的非常高效的卷积神经网络。

LeNet-5 特点：

1. 使用卷积提取空间特征
2. 使用空间均值映射进行下采样
3. 使用S型曲线形式的非线性单元
4. 多层神经网络作为分类器
5. 层与层之间使用稀疏连接降低计算量

## AlexNet

For deep neural networks to shine, they needed far more labeled data and hugely more computation. -- Alex Krizhevsky

AlexNet是Hinton小组在ISVRC2012中获得了第一名时使用的神经网络模型,以第一作者Alex Krizhevsky的名字来命名,论文题目为ImageNetClassification with Deep Convolution Neural Networks, top5测试错误率是15.3%,第二名是26.2%

在此之前,神经网络处于机器学习的大们鄙视链的底端,当时整个计算机视觉领域普遍的看法是计算机视觉系统需要充分地理解任务的类型,通过人为的、细致的调整来提高分类的精确度。

AlexNet 特点：

1. 更深的神经网络层数
2. 使用ReLU作为激活函数
3. 多GPU并行训练
4. 重叠最大池化
5. LRN层(现在很少被使用)

## VGG

VGG (Visual Geometry Group)

VGGNet 是牛津大学计算机视觉组和 DeepMind 公司共同研发一种深度卷积网络(论文《Very deep convolutionalnetworks for large-scale image recognition.》) ,并且在 2014 年在 ISVRC 比赛上获得了分类项目的第二名和定位项目的第一名一共有 6 种模型,表现最好的是 VGG-16 和 VGG-19

VGG 特点：

1. 深，
2. 更深
3. 小卷积核
4. 小池化核
5. 全连接转卷积 (最后三层FC变成7\*7\*4096+1\*14096+1\*1\*1000)

小卷积核的优势

1. 更多的激活函数，更丰富更细致，更强的识别能力
2. 在保证感知野不变的情况下，参数更少，更强的性能

1\*1卷积核的作用

1. 实现跨通道的交互和信息整合
2. 进行特征图的降维和升维
3. 在不改变 feature map 尺寸的前提下,增加非线性单元,提升网络深度

## ResNet

残差网络, Residual Network,出自MSRA何凯明的论文《DeepResidual Learning for Image Recognition》 , 2015年的 imagenet 比赛中斩获多个方向冠军,被评为 CVPR2016 的 best paper

ResNet主要面向是深度网络的"退化”问题,即当网络层数加深到一定程度时,错误率反而提高

根据泛逼近定理(universal approximation theorem) ,如果数据量和神经元足够大,单层的前馈网络也可以去拟合任何函数。但是,该层可能非常庞大,网络和数据易出现过拟合。因此,研究界普遍认为网络架构需要更多层。AlexNet:8，VGG: 19，GoogleNet: 22

为什么会"退化"层数加深,网络更复杂,学习能力更强,拟合能力更强,为什么会退化呢?

其实不是过拟合问题，而是梯度消失问题

一个小于1大于0的小数，学习很多次后，就趋近于0，这也是梯度消失的原因，通过残差处理来解决