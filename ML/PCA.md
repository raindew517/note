# PCA

Everything that has a start, has an end. -- Oracle from \<Matrix\>

PCA(**Principal component analysis**)

“凡滥之类妄”，其实维度太多了也没有什么用

随着维度的增加，会给数据带来更多的信息，但是太多了只会带来冗余

## 数据的维度

### 样本的稀疏性

随着维度的增加，样本之间的距离也会变大

样本空间越来越大，同时样本就越来越稀疏

我们在给定边界内是需要一定数量的样本才能够正确的构建模型的，高维空间会导致严重的过拟合

如果维度增加，样本数量也应该增加，不然就很容易过拟合

### 样本的分布

样本空间（hypercube）、样本空间中心（hypersphere）

在hypercube边缘处的样本一般叫“离群点”或者“噪点”，不容易学习

只有离样本空间中心更近才更容易学习
$$
超球体的体积公式\\f_d=\frac{Vol_{sphere}}{Vol_{cube}}=\frac{\pi^{\frac{d}{2}}}{2^d\Gamma(\frac{d}{2}+1)}
$$
维度越高，球体占方体的比例就越小

所以在高维空间，基本上样本都处在边缘，这样学习的话效果是很差的

#### 概率分布

在自然情况下，概率分布应该处于一个高斯分布的情况

但是高维的情况下，概率分布就变得畸形

## 降低数据的维度

一般是从这三个方面入手：

- 数理和信息特征
- 特征本身的特性
- 卷积和池化

### 特征本身的特性

在树模型中，我们会以特征的重要性来筛选特征

还有其他的几个方面来衡量特征

- 卡方
- 余弦相似度
- 缺失值过多
- 相关性过大

### 卷积和池化

属于深度学习

### 数理和信息特征

有以下方法，但重点讲 PCA

- PCA
- LLE
- ICA
- T-SNE

PCA 是很早就提出来了的一个方法，它对信息本身的利用是非常到位的，时至今日它都是很常用的信息处理方法，同时也是无监督学习当中非常重要的方法

如果有两种特征池，一种是包含非重要特征和重要特征的，一种是没什么特别重要的，都一般般的，PCA 就是不管是哪种特征池，都**尽可能合并信息，在合并过程中尽可能少丢失信息（不是挑选特征）**

合并信息后的特征池，注意这里的特征是新构造的特征，已经和原特征完全不同

合并之后的特征会按信息携带量排序

投影的结果就叫做主成分(PC)

沿着不同方向向量投影，得到的主成分所携带的信息量(方差)是不同的

如何找到携带信息量最多的那些投影方式，就是PCA的目的

（使得变换后行向量间的协方差为0，每个行向量自身的方差尽可能大）

#### PCA 的具体过程

1. 去中心：每个特征（维度）减去该维度的均值 
2. 求协方差矩阵 
3. 计算协方差矩阵的特征值和特征向量 
4. 对3的特征值结果排序 
5. 选择最大的的k个特征值对应的特征向量 
6. 原数据矩阵与特征矩阵相乘投影出新的数据

主要是第二步和第三步的协方差矩阵、特征值和特征向量的概念和计算

#### 协方差矩阵

数学期望
$$
E(X)=\int_{-\infty}^{+\infty}xf(x)dx
$$
方差
$$
D(X)=E(X-E(X))^2
$$
方差本来是说明数据对于数据中心的离散程度，在信息学当中，方差越大，说明数据蕴含的信息就越多，但其实这个信息是好是坏我们是不知道的

协方差
$$
Cov = E(X-E(X))E(Y-E(Y))
$$
协方差反应的是两组数据（两个特征）之间的相关性

在机器学习特征当中，我们更加喜欢更加相互独立的变量

协方差矩阵就是将特征之间的相关性两两相比，并用矩阵的方式呈现

#### 特征值和特征向量

特征值

投影 $ux=u^Tx$

我们只对投影的方向感兴趣，对对应向量的模不感兴趣，所以默认为u是单位向量

投影的方差
$$
\frac{1}{n}\sum_{i=1}^n(u^Tx_i)^2=u^T\frac{1}{n}\sum_{i=1}^nx_ix_i^Tu\\
这里的\frac{1}{n}\sum_{i=1}^nx_ix_i^T记作S
$$
最终要求的表达式(求x由u投影后的最大方差)
$$
\arg\max\limits_{u}u^TSu\\
拉格朗日乘子法、奇异值分解法\\
Su=\lambda u\\
u^TSu=u^T\lambda u=\lambda\\
u是单位向量，所以特征值就是方差
$$

### 相关 api，超参数

超参数

*# n_components = 50* 

*# 当值为整形的时候，即表示保留的PC数量的多少*

*# 当值为字符型的时候，可以设置"mle'，即让sklearn自动决定保留多少个PC*

*# 当值为float(0,1)之间的时候，设置保留对应信息量的PC，如0.75就表示对应百分之75信息的PC数量被保留*

*# whiten = Flase，是否启用数据白化，启用白化会带来一些准确率，有时也会丢失一些信息，PCA就是丢失信息，白化会继续丢失信息，所以一般是不启用的*

*# svd_solver = 'auto'，设置奇异值分解的求解方法,'auto', 'full', 'arpack', 'randomized'*

在计算机中，奇异值分解求得比较快，所以不用拉格朗日乘子法

*### 返回值*

*# components_ 返回PC*

*# explained_variance_ 返回PC对应的方差*

*# explained_variance_ratio_ 返回PC对应的方差(以百分比形式)*

*# n_components_ 当选择由sklearn自行决定PC数量时，返回得到的PC数量*

## 案例

人类基因，有大量的特征，而且其中也有很多是没有用的

现实生活中的数据很多是稀疏矩阵的，所以用 PCA 挺好的

如果数据特征之间的相关性很高是好用的，但如果相关性不大，那么数据降维就会丢失大量信息，效果会变得很差

实际使用 PCA 也不多，一般还是用树去筛选，不过 PCA 可以作为备选方案

