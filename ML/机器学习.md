## 概述

算法貌似早就有了，但是当时没有那么多的数据，也没有算力

人工智能包括机器学习（广义），机器学习（广义）包括深度学习

还是要学习经典机器学习算法，因为不同的每一个问题它对应的最优解

计算机视觉和自然语言领域大量使用深度学习算法

而处理结构化数据，在金融，医疗领域大多使用经典机器学习算法

## 监督学习与无监督学习

**监督学习**，样本(Feature)和标签(Label)可以是离散型也可以是连续型

离散型的是分类任务，连续型的是回归任务

监督学习是以任务为驱动的方法，也是机器学习、深度学习领域应用最广泛的方法类型

**非监督学习** 可能没有 label，以数据为驱动去聚类然后总结规律，也有很多应用方向

## 训练、验证、测试与评估

训练阶段和测试阶段

**训练阶段**就是要去总结数据的信息，从中学习有用规律的过程

**验证和测试**都是检验训练与结果的途径

**验证**不参与模型参数的求解，但是参与模型的学习过程，更多的是**属于训练过程**

**测试**与训练过程和模型绝缘，用来苹果模型的最终效果，**属于测试阶段**

验证像是课后作业参与到了学习的过程，测试像是期末考试，考试里面的详细内容是事先不知道的，通过考试得到最终的反馈

**评估**包括预测正确的情况如何？预测错误的情况如何？这个模型到底是好是坏？

## KNN

KNN(K-Nearest Neighbors, K近邻算法)，机器学习中最简单的算法

“离谁近，就是谁”

E. Fix and J. L. Hodges(1951): An important contribution to nonparametric discriminant analysis and density estimation: Commentary on Fix and Hodges

**距离度量**

欧氏距离(L2)，就日常的两点之间的距离
$$
\sqrt{\sum_{i=1}^k(A_i-B_i)^2}
$$
曼哈顿距离(L1)，沿着空间坐标轴行径的长度
$$
\sum_{i=1}^k|A_i-B_i|
$$
明氏距离(Lq)
$$
({\sum_{i=1}^k(A_i-B_i)^q})^{1/q}
$$
明氏距离在q=1时就是曼哈顿距离

明氏距离在q=2时就是欧氏距离

一般欧氏距离就够用了

曼哈顿距离的使用场景（也可能存在争议）：

1. 求解过程中使得离群点的影响更小
2. 维度极高的数据（计算开销比欧氏距离小）
3. 更适合数据没有归一化/不平衡

算出距离后取距离最小的前k个，根据它们的 label 比例来判断

k 一般取奇数，小于20

## 结果的指标与评估

一般将原始数据（标签）划分为训练集（标签）和测试集（标签）

训练集构建模型后，测试集也是带标签的，用来检验模型是否正确

**分类模型的评估**

混淆矩阵 confusion matrix

|           |          | actual         | actual         |
| --------- | -------- | -------------- | -------------- |
|           |          | positive       | negative       |
| predicted | positive | true positive  | false positive |
| predicted | negative | false negative | true negative  |

真正（true positive，TP） --- 有病，查出来了

假正（false positive，FP）--- 没病，说你有病

假负（false negative，FN）--- 有病，没查出来

真负（true negative，TN）--- 没病，检测正常

**Accuracy 准确率**
$$
\frac{TP+TN}{TP+TN+FP+FN}，正确数预测的正反例数/总数
$$
**Precision 精确率**
$$
\frac{TP}{TP+FP}，正确预测的正例数/预测正例总数
$$
**Recall 召回率**
$$
\frac{TP}{TP+FN}，正确预测的正例数/实际正例总数
$$
**F1-score** 值越大越好
$$
\frac{2*Precision*Recall}{Precision+Recall}
$$
还有两个指标 ROC 和 AUC

## 线性回归

比如车辆重量和车油耗的关系就是典型的回归问题

线性回归就是样本空间当中对样本规律的线性总结

$h_\theta(x)=\theta_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n$
$$
\theta=
\left[
\begin{array}
{ccc}
	\theta_0\\
	\theta_1\\
	\theta_2\\
	\vdots\\
	\theta_n
	\end{array}
\right]
X=
\left[
\begin{array}
{ccc}
	1\\
	X_1\\
	X_2\\
	\vdots\\
	X_n
	\end{array}
\right]
$$
$h_\theta (x)=\theta^TX$

n=1时，一元线性回归，回归直线

n>1时，多元线性回归，回归平面/超平面

## 损失函数与目的

预测值和真实值是有误差的，要所有误差的平方和大概就是损失函数了

找到合适的θ，使得回归方程能最好的总结数据的线性规律

找θ的两个方法：正规方程和梯度下降，它们都依赖损失函数

损失函数不用绝对值的原因是绝对值函数在x=0是是不能求导的，梯度下降就用不了了

损失函数
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(\theta_0+\theta_1x_1^{(i)}-y^{(i)})^2，这里的上标(i)是第i个样本的意思
$$
回归直线的形状取决于θ

line of best fitting：找到合适的θ，是的回归直线/平面/超平面能最好的描述数据的规律
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(\theta^Tx^{(i)}-y^{(i)})^2
$$
在损失函数 J(θ) 最小时，回归直线的总体误差最小，回归直线表现即最好，所以求最佳回归直线对应的θ值的问题被转化为求 J(θ) 的最值

## 正规方程

也叫最小二乘法，就是求$J'(\theta)=0$的时候

先将 J(θ) 表示成$J(\theta)=\frac{1}{2}(\theta^TX-Y)^T(\theta^TX-Y)$的形式（我黑人问号就来了）

因为$X\theta=Y$，得$\theta=X^{-1}Y$就可以求出θ了

但这是有前提的，就是X存在逆矩阵$X^{-1}$，但前提得是方阵才有可能有逆矩阵，因此，通过左乘一个$X^T$是的等式变成$X^TX\theta=X^TY$，又得$\theta=(X^TY)^{-1}X^TY$，但$(X^TY)^{-1}$还是有可能不存在的，不过它极少不存在，即使不存在也有处理方法

## 梯度下降

正规方程计算量大，样本很多的时候用起来就不太理想了，$(X^TX)^{-1}$不满秩也无法使用，感觉也没有学习的过程

机器学习、深度学习中最重要的优化/求解方法，没有之一
$$
\theta_1^{(p+1)}=\theta_1^{(P)}-\alpha\frac{\partial J(\theta_1^{(p)})}{\partial \theta_1^{(p)}}\\
\theta_1^{(p+1)}=\theta_1^{(P)}-\alpha\bigtriangledown(\theta_1^{(p)})
$$
α就是学习率，它来控制每次下降的步伐大小，α后面的就是梯度

学习率过小就会导致迭代次数过多

学习率过大，可能会在最小值附近来回

学习率非常大的时候，可能会出现无法收敛的情况

学习率一般在0.1到0.01之间

使用全部数据计算梯度，批量梯度下降 Batch Gradient Descent 

使用单个数据计算梯度，随机梯度下降 Stochastic Gradient Descent

真实的情况应该是使用小批数据计算梯度，小批量梯度下降 Mini-Batch Gradient Descent

## 多项式回归

多项式回归可能会出现以下两个问题：

**过拟合** over fitting 

如果在二维坐标系中，过拟合大概就是曲线经过了所有的样本点

**过拟合是对样本机械的记忆，并没有总结出样本的一般规律，记忆而不是学习**

“火候过大，东西烧焦了”，训练集效果爆表，测试和使用中一落千丈

这个时候就需要正则化

**欠拟合** under fitting

**欠拟合是对样本规律的总结的欠缺，没有学习到位**

“火候不够，东西还没熟”，无论在训练集，测试集还是使用上，效果都很差

这个时候就要更换模型或者继续训练

## 正则化

令高次项对应的θ尽可能小

有两种比较主流的方法：

岭回归 Ridge Regression，L2 正则，在原来的损失函数加上$\frac{\lambda}{2}||\theta||^2$
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(\theta^Tx^{(i)}-y^{(i)})^2+\frac{\lambda}{2}||\theta||^2
$$
尽可能的让所有的特征θ值更小进而减小损失函数

LASSO回归 least absolute shrinkage and selection operator，L1 正则，$\frac{\lambda}{2}||\theta||$

使得某些不重要的特征对应的θ趋向于0，进而减小损失函数，可以用来进行特征选择

## 回归的评估方法

绝对平均误差 MAE，但是大家对绝对值的使用有偏见
$$
MAE=\frac{1}{m}\sum_{i=1}^m|y_{true}^{(i)}-y_{predict}^{(i)}|
$$
使用的比较最多的是均方差/根均方差，MSE/RMSE (Root Mean Squard Error)，像损失函数的变种
$$
MSE=\frac{1}{m}\sum_{i=1}^m(y_{true}^{(i)}-y_{predict}^{(i)})^2\\
RMSE=\sqrt{\frac{1}{m}\sum_{i=1}^m(y_{true}^{(i)}-y_{predict}^{(i)})^2}
$$

MSE 是对整体数值的把握，RMSE 是让数据量级拉回到原来的数据量级之上，MSE计算量可能小点，需要平衡量级就要去用 RMSE

R squared 越大越好
$$
R^2=1-\frac{MSE}{VAR}\\
1-\frac{\frac{1}{m}\sum_{i=1}^m(y_{true}^{(i)}-y_{pridect}^{(i)})^2}{\frac{1}{m}\sum_{i=1}^m(y_{true}^{(i)}-y_{true}^{mean})^2}
$$
看到了表示1/m不是应该给消掉吗？

### 相关的API与超参数

sklearn 中各种回归中 normalize 标准化，在绝大多数情况下我们认为在线性回归的梯度下降中，标准化是必要的，除非想要突出某些特征

在岭回归和 Lasso 回归中，alpha 代表正则化程度，alpha 越大表示正则程度越强

## 逻辑回归

