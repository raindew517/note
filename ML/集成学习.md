# 集成学习

若不团建，任何力量都是弱小的 -- Jean de la Fontaine

本章要求了解基本的树算法分类与回归

## 强分类器与弱分类器

strong classifier 和 weak classifier 是相对的概念，不是以模型或者算法的复杂程度划分的，是以具体的效果来划分的，我们依据学习器的效果，将某个特定任务的学习器分为强学习器和弱学习器

## bagging 与随机森林

bootstrap 是重采样的思想，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想是从原样本集有放回的抽取 N 个子集，训练 N 个分类器，进行集成。

Bagging 是 Bootstrap Aggregating 的缩写，Bagging是一种在原始数据集上通过有放回抽样重新选出S个新数据集来训练分类器的集成技术。也就是说这些新数据集是允许重复的。使用训练出来的分类器集合来对新样本进行分类，然后用多数投票（在一系列分类边界的中属于哪一类多就最终判定属于哪一类）或者对输出求均值的方法统计所有分类器的分类结果，结果最高的类别即为最终标签。

在 bagging 下有个著名的算法，就是算计森林算法，随机森林就是生成很多的决策树，不是将数据集做 bootstrap 再构建树然后组成森林，而是**在每次分裂的时候对整体进行取样，直接用整个数据集构建随机森林，使用没被选中的数据做 validation，不进行剪枝**（大量的决策树解决了过拟合的问题，不需要剪枝）

除了不太适合小样本学习，随机森林是很优秀的，不过不适合小样本学习是几乎所有树算法的缺点

随机森林是学习能力比较强，也是实际生产活动中比较主流的算法

## Boost 算法原理

为什么是随机森林，而不是其他的算法？

随机森林首先是一个树算法，集成学习是从弱学习器到强学习器的一个过程，这个过程除了树学习器，其他任何的学习器都可以使用，但是为什么最后用了随机森林而不是其他呢？

variance 方差反应的就是预测结果与整体数据的离散差异程度，说人话就是是不是更聚拢，过拟合一般也是高方差的问题

bias 偏差就是预测结果离想要的结果的距离，说明预测是不是对了

噪音误差是由数据本身带来的，不可避免，我们能去做到的就是降低方差和降低偏差

决策树是非常敏感的，给它什么样的数据就会得到什么样的结果，即使数据的差异非常的小也能得到不同的结果，决策树的结果就是方差比较高的结果，用 bagging 就可以去降低方差，这就逐渐变成了随机森林

所以

bagging 会去纠正高方差，树分类器就会去将结果给聚拢起来

boosting 就是处理偏差比较大的问题

在 bagging 算法中，我们是采用的并行投票的机制，而 boosting 不一样，它是串行的一个算法，每一个分类器是在前一个分类器的计算的基础之上进行构造的，从而去构造一个强分类器

根据发布的时间先后，主要有 Adaboost，GBDT，LGB/XGB/CAT，准确来说，LGB/XGB/CAT 不算新的算法，它们应该说是 GBDT 的更高效的实现，跟随机森林是 bagging 的更高效的实现是一个道理

## Boosting 算法初阶：Adaboost 算法原理

集成算法指的是 boost 算法

Adaboost，是英文"Adaptive Boosting"（自适应提升/增强）的缩写

加法模型：每一个分类器都是由前一轮的分类器加上当前轮次的分类器组成的

向前分布：向前累加

指数损失函数：在现代的 adaboost 算法中我们也会使用别的损失函数

核心思想：

1. 让更优秀的基分类器占更大权重
2. 在处理过程中，更加注重处理难处理的样本

$$
H(x)=\sum_{i=1}^T\alpha_ih_i(x)\\
\alpha权重，h是弱分类器\\
最后会得到强分类器
$$

求救的核心就是去求权重和每一轮的弱分类器

在求解权重的时候，它每次只求一个权重，然后逐渐向前求解

弱分类器 h(x)

经典的 Adaboost 中采用的默认分类器为 stump ( 树桩 ) 分类器，即只有一个节点的决策树，并不支持多分类与概率预测

下面是经典的一个 Adaboost 的流程：
$$
Dataset:(x_1,y_i)...(x_m,y_m),y\in\{1,-1\}\\
使用数据权重分布\ D_t\ 训练出最优的基分类器，并得到加权错误\\
\varepsilon_t=Pr_{i\sim D_t}[h_t(x_i)\ne y_i]\\
依据加权错误 \varepsilon_t 计算分类器权重 \alpha_t\\
\alpha_t=\frac{1}{2}In(\frac{1-\varepsilon_t}{\varepsilon_t})\\
迭代强分类器\ H(x)\\
H(x)=\sum_{i=1}^T\alpha_ih_i(x)\\
更新下一轮的权重\ D_{t+1}\ 以备使用\\
D_{t+1}=\frac{D_{t}exp(-\alpha_ty_ih_t(x_i))}{Z_t}
$$
跟 ID3 算法有点像，不过 Adaboost 多了一个权重，上面的流程不断的重复

现代的 Adaboost 算法，使用的弱分类器为 CART（Classification and Regression Tree），cart 本身支持多分类，支持概率预测，甚至可以做回归，所以用 cart 分类器作为基分类器

当我们使用 CART 的时候，不需要去分特征计算 et，CART 树产生的结果就可直接算出 et，这里的 e 就是 ξ，是因为编程的时候不会去打这个符号

传统 boost 算法不一定是越迭代就越精准，以 Adaboost 为代表的传统 boost 算法，不一定每一轮 boost 都会向最优的方向提升

## GBDT

GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，是 boost 算法中比较进阶的算法

**通过梯度来确定优化的方向使优化的方向总是正确**
$$
f(x)=\sum_{m=1}^MT(x;\Theta_m)\\
f(x)=T_1(x)+T_2(x)+....
$$
这里没有权重了，看起来好像是变回了 bagging 算法，但其实不是的

初始值也不一定是 0

$F_x(x)$ 中的每一个树，都是回归树，使用回归思想解决分类问题，使用连续变量逐渐逼近结果的一个原理

例如：
$$
F_3(x)=T_1(x;\Theta_1)+T_2(x;\Theta_2)+T_3(x;\Theta_3)\\
100=85+10+5
$$
初始状态：目标值100，初始值0，差值100

轮次1：目标值100，预测值85，差值15

轮次2：目标值15，预测值10，差值5

轮次3：目标值5，预测值5，差值0

GBDT 中，每一轮的输入值是上一轮目标值与预测值的差值，这个差值也成为了本轮的预测的目标，往复迭代

GBDT 不光可以从正方向去逼近，反方向也是可以的，100=105-3-2

来衡量优化的好坏还是使用损失函数
$$
F_m(x)=F_{m-1}(x)+T(x;\Theta_m)
\\L(y,F_m)=(y-F_m)^2=(y-(F_{m-1}+T(x;\Theta))^2= (y-F_{m-1}-T(x;\Theta))^2\\
res_m=y-F_{m-1}(x)\\
原来的论文使用的是近似值梯度去表示残差
-res_{mi}=-\bigg [\frac{\partial L（y_i,F(x_i)}{\partial F(x_i)}\bigg ]\\
回归当中 res_m=差值\\
分类当中 res_m=概率的差值
$$
计算的时候基分类器还是使用 cart，GBDT 全部是回归树，而不是分类树，所以用 cart

样本是类别 J 的概率为
$$
P_{mj}=\frac{exp(F_{mj}(x;\Theta))}{\sum_{j=1}^Jexp(F_{mj}(x;\Theta))}
$$

### shrinkage

每轮的强分类器是由前一轮的强分类器加上本轮的树所组成的，每轮所学习的树是可以去期待的，如果每轮学习的步子太大就会错过最优解得情况，这跟梯度下降中的学习率有点像，但不是一回事，在 GBDT 中，我们也设置了一个学习比率叫 shrinkage，这个比率通常是一个比较小的数值，这里也体现了 GBDT 的一个思想，“小心驶得万年船”，意思是我们每一轮的树只从前一轮中学习一小部分，就这样小心的前行，保证不会错过最优解
$$
F_m(x)=F_{m-1}(x)+vT(x;\Theta_m)\\
v就是学习比率
$$
当然这也不是没有缺点，每次学的太少了就会导致迭代次数太多了，为了平衡这个比率的数值一般是 0.01、0.001 之间，当然也不一定，具体情况具体分析

GDBT 其实是比较优秀的算法，因为它的思想一直到现在在工业中的运用都是比较多的

优点：

- 预测精度高
- 处理数据相对较快（与传统 boost 相比，使用更少轮次，因为梯度总是指向正确的方向）
- 非常适合非线性数据处理

缺点：

- 不能并行计算（后期通过数据结构可改进）

## LGB、XGB、CAT

高精度机器学习商业应用主力军

GBDT 的具体高级实现（是在梯度的思想之上，增添了一些结构，存储，计算方面的优化）

XGB 是这三个当中最早诞生的，2014 年，Xgboost(eXtreme Gradient Boosting) 由华盛顿大学的 TianQi Chen 提出，作为研究项目，它的优化手段可能没有后面新出现的算法多，但是因为最早提出，所以现在这种算法的应用是最广的

LGB 是由微软 2016 年底提出，2017 年发布的

CAT 是由俄罗斯巨头 Yandex 在 2017 年提出的，CatBoost 是 Category 和 Boosting 的缩写，最大的特点就是可以直接处理类别特征，不需要任何预处理来将类别转换为数字。

后两种是由商业公司提出的，所以它们是更面向商业的

前面提到的 GBDT，或者说在树算法中，有两点是比较重要的，一个是**特征筛选，节点分裂**，另一个是**特征预处理**，如果要去优化，也是要从这两方面去入手

### 特征筛选，节点分裂

对于每一个特征的每个潜在划分点，计算
$$
MSE=\sum_{i=1}^m(f(x^{(i)})-c^{(i)})^2
$$
当特征量十分巨大，样本量十分巨大时，这样的过程十分耗时

之前的 GBDT 是在每一个特征之间都进行一次分裂，然后去计算 MSE，

### histogram

上面三种算法都选择了 **histogram** 的方法，选择一段区域的特征去划分，区域越少，计算量就越少，这样的计算精度就下降了

很显然我们一般会去等分这样的扇区，但是在面对不平衡样本的时候，这样的划分会带了整个树的不平衡，所以我们需要去选择最优的划分节点，那么如何选择最优并的数量以及并的位置就是个问题了

在 XGB 中是用的 Weighted Quantile Sketch
$$
\sum_{i=1}^n\frac{1}{2}h_i(f_t(x_i)-g_i/h_i)^2+\Omega(f_t)+constant
$$
通过二阶导数去判定哪些位置，哪些节点是应该划分的，哪些节点不应该划分的，在 GBDT 中我们用的是一阶导数，而这里的二阶导也叫牛顿法，可以看到当前最优，还能看到下一步最优

### sparse input

在面对大规模数据的时候，数据通常比较稀疏，数据里面会有大量的 0，这些 0 可能就是一些冗余信息，会干扰节点的划分，三种算法都不约而同的选择了忽略这些 0 去进行划分，将稀疏矩阵中的 0 都进行移除，然后再进行划分，这样比较简单，因为排除的 0 项的干扰

### GOSS

在 LGB 和 CAT 中比较独特的方法 GOSS（Gradient-based One-Side Sampling）就是单边梯度法，它的核心思想是，在一套数据中，有一些数据对模型的构建是由巨大帮助的，但是还有另外一部分数据对构建没有帮助或者说是帮助非常小，如果能去这样的区分数据，那训练的效率就非常高了

在离全局最优越近，它的梯度越小，离全局最优越远，它的梯度越大，根据梯度的计算，我们就可以去鉴别重要的和不重要的数据点，然后就可以把精力更多的投入到重要的点上面，从而使得整个计算效率提升

这里也会有个问题，只选择梯度大对应的数据点可能会导致 bias 增加，在 LightGBM 中，会根据实际情况，使用大部分大梯度数据和随机小部分小梯度数据，虽然小部分小梯度对于模型收敛的速度没有那么重要，但是它可以用来平衡大梯度数据产生的 bias

在 XGB 中是不使用单边梯度法的

### EFB

在 LGB 中还有一个独有的叫 EFB（Exclusive Feature Bundling），某些非零元素可能一辈子都不可能在一起，举个例子：“铝合金”和“心理学”，我们都几乎看不到这两个词同时出现，然后我们就可以把此类特征融合成一个特征，降低特征数量，这个特别适用于高维的稀疏矩阵

| 13   | 0    | 0    | 51   | 12   | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 0    | 0    | 12   | 0    | 0    | 98   |
| 13   | 0    | 12   | 51   | 12   | 98   |

一二行就合并了

### 并行计算

这三种算法都可以进行并行计算，这可能也是这三种算法能够顶替 GBDT 的很重要的一个原因，因为在商业的环境当中，计算效率是很重要的

boost 算法其实还是串行计算，但是我们可以在特征层面进行并行，不是一次处理一个特征再处理下一个，而是通过 block 结构同时处理多个特征

### 生长方式

xgb 树的生长方式是 level-wise growth，一层一层的生长，可保证这是个生长平衡的树

lgb，cat 是 leaf-wise 生长，会先逮住一个节点，知道分裂到这个节点的最终，可以将所有的特征分裂到极致，这样的比较准确，不过也有过拟合的潜在风险

### 类别变量

三种算法都有类别变量

cat 的类别变量是这三种中最好的，
$$
\frac{\sum_{j=1}^{p-1}[x_{\sigma_j},k=x_{\sigma_p},k]Y_{\sigma_j}+\alpha P}{\sum_{j=1}^{p-1}[x_{\sigma_j},k=x_{\sigma_p},k]+\alpha}\\
target\_value=\frac{classcount+p}{totalcount+1}\\
p是先验概率
$$
这样类别变量可以保证整个类别转换有一个差异性，如果样本中有大量的类别变量，catboost 就是很适合的一个算法

上面都是一些简单的总结，还没涉及到复杂的数学原理

## 

对于xgb，是默认忽略 0 对待的

对于lgb，需设置超参数 zero_as_missing = True use_missing = True

## 相关的框架，API 与超参数

现在市场上用的多的就是 CatBoost LightGBM XGBoost

在那些出名的数据集中的表现没有什么太大的区别

大规模数据首选 LGB

XGB 实例多，应用多

CAT 类别变量多首选

用起来都差不多，LGB 的 api 和设计更舒服一些，CATBoost 的潜力很大

有时候，简单的算法和复杂的算法只是精度的差别，其实得到的结果是差不多的，但是复杂的算法可能效率没有那么高，这种情况下我们会选择简单的算法，没有必要强行去使用某种算法，具体的情况具体分析，用最合适的就行

这三种算法是由各自的团队提出的，所以也是在各自的包中，adaboost 和 GBDT 在 sklearn 中有提供

### 超参数
learning_rate = 0.001，学习率或步长，越大收敛速度越快，但可能精度下降,eta = 0.001
n_estimators=10000，boost的轮次，越大可能得到的精度越高，但也可能出现过拟合风险，并且计算越慢，我们实际使用中有别的技巧   
n_estimators=10000，在随机森林中也是同样的参数，只不过是表示树的数量，也是bootstrap的次数

#### 这部分基本都是决策树中大家熟悉的超参数
min_child_weight = 0
colsample_bytree = 0 
max_depth = 0
subsample = 0
min_split_gain=0
min_child_weight=0

#### 正则项lgb
reg_lambda = 0
reg_alpha = 0
#### 正则项xgb
alpha
gamma

#### 正则项cat
l2_leaf_reg
l1_leaf_reg

#### 独有超参数
cat_features = True , one_hot_max_size  = 50 （catboost)
categorical_feature = None (lgb)
bins = 9 (xgb)

## 总结

- 强分类器与弱分类器
- bagging 原理与算法
- 随机森林算法
- **偏差与方差**
- bagging 与 boost 背后数学思想
- adaboost
- GBDT
- lgb
- xgboost
- catboost

